{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubic\n",
    "Grading Rubric\n",
    "* **[Data Preparation](#Data_Preperation)** (15 points total)\n",
    "    * **[10 points]** Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "    * **[5 points]** Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "* **[Modeling and Evaluation](#Modeling_and_Evaluation)** (70 points total)\n",
    "    * **[10 points]** Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up\n",
    "any assertions.\n",
    "    * **[10 points]** Choose the method you will use for dividing your data into training and testing splits\n",
    "(i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "    * **[20 points]** Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "    * **[10 points]** Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "    * **[10 points]** Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "    * **[10 points]** Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "* **Deployment (5 points** total)\n",
    "    * **How useful is** your model for interested parties (i.e., the companies or organizations that might\n",
    "want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\n",
    "* **Exceptional Work (10** points total)\n",
    "    * **You have free** reign to provide additional analyses.\n",
    "    * **One idea:** grid search parameters in a parallelized fashion and visualize the performances across\n",
    "attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduciton\n",
    "Explanation of our data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation<a id=\"Data_Preperation\"></a>\n",
    "\n",
    "\n",
    "### [Pre Processing](#Pre_Processing)\n",
    "1. Set appropiate data types\n",
    "2. Removed columsn that may be problomatic\n",
    "    * Columns with same value in all rows\n",
    "    * Columsn with a unique value in all rows\n",
    "    * Empty Columns\n",
    "3. Remove columsn with low fill rate or high frequency of same value\n",
    "4. Missing values\n",
    "    * Update Census Continous values\n",
    "    * Update Census Categorical values\n",
    "    * Update remaining values\n",
    "5. Remove Duplicate Rows\n",
    "6. Data Quality Clean-up\n",
    "7. Create new features\n",
    "\n",
    "### [Feature Seleciton \\ Model Building](#Feature_Selection_Modeling_Building)\n",
    "1. Create One-Hot encoding on variables used for modeling\n",
    "2. Remove columsn with similar characterisitcs or a corrleation more than .95\n",
    "\n",
    "### [Data Definitions](#Data_Definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.03 s, sys: 401 ms, total: 5.43 s\n",
      "Wall time: 5.74 s\n",
      "CPU times: user 1.62 ms, sys: 1.37 ms, total: 2.99 ms\n",
      "Wall time: 3.14 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%time malware = pd.read_csv(\"data/final.csv\")\n",
    "%time data_glossary = pd.read_csv(\"data/data_glossary.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_column_array(cols):\n",
    "    cols_booleans = [\n",
    "        \"IsBeta\",\n",
    "        \"IsSxsPassiveMode\",\n",
    "        \"HasTpm\",\n",
    "        \"IsProtected\",\n",
    "        \"AutoSampleOptIn\",\n",
    "        \"PuaMode\",\n",
    "        \"SMode\",\n",
    "        \"Firewall\",\n",
    "        \"UacLuaenable\",\n",
    "        \"Census_HasOpticalDiskDrive\",\n",
    "        \"Census_IsPortableOperatingSystem\",\n",
    "        \"Census_IsFlightingInternal\",\n",
    "        \"Census_IsFlightsDisabled\",\n",
    "        \"Census_ThresholdOptIn\",\n",
    "        \"Census_IsSecureBootEnabled\",\n",
    "        \"Census_IsWIMBootEnabled\",\n",
    "        \"Census_IsVirtualDevice\",\n",
    "        \"Census_IsTouchEnabled\",\n",
    "        \"Census_IsPenCapable\",\n",
    "        \"Census_IsAlwaysOnAlwaysConnectedCapable\",\n",
    "        \"Wdft_IsGamer\"\n",
    "    ]\n",
    "\n",
    "    cols_categorical = [\n",
    "        \"ProductName\",\n",
    "        \"EngineVersion\",\n",
    "        \"AppVersion\",\n",
    "        \"AvSigVersion_x_x\",\n",
    "        \"RtpStateBitfield\",\n",
    "        \"AVProductsInstalled\",\n",
    "        \"AVProductsEnabled\",\n",
    "        \"CountryIdentifier\",\n",
    "        \"OrganizationIdentifier\",\n",
    "        \"Platform\",\n",
    "        \"Processor\",\n",
    "        \"OsVer\",\n",
    "        \"OsBuild\",\n",
    "        \"OsSuite\",\n",
    "        \"OsPlatformSubRelease\",\n",
    "        \"SkuEdition\",\n",
    "        \"SmartScreen\",\n",
    "        \"Census_MDC2FormFactor\",\n",
    "        \"Census_DeviceFamily\",\n",
    "        \"Census_ProcessorManufacturerIdentifier\",\n",
    "        \"Census_ProcessorClass\",\n",
    "        \"Census_PrimaryDiskTypeName\",\n",
    "        \"Census_ChassisTypeName\",\n",
    "        \"Census_PowerPlatformRoleName\",\n",
    "        \"Census_InternalBatteryType\",\n",
    "        \"Census_OSArchitecture\",\n",
    "        \"Census_OSBranch\",\n",
    "        \"Census_OSBuildNumber\",\n",
    "        \"Census_OSEdition\",\n",
    "        \"Census_OSSkuName\",\n",
    "        \"Census_OSInstallTypeName\",\n",
    "        \"Census_OSInstallLanguageIdentifier\",\n",
    "        \"Census_OSUILocaleIdentifier\",\n",
    "        \"Census_OSWUAutoUpdateOptionsName\",\n",
    "        \"Census_GenuineStateName\",\n",
    "        \"Census_ActivationChannel\",\n",
    "        \"Census_FlightRing\",\n",
    "        \"Wdft_RegionIdentifier\"\n",
    "    ]\n",
    "\n",
    "    cols_categorical_large = [\n",
    "        \"AvSigVersion\",\n",
    "        \"DefaultBrowsersIdentifier\",\n",
    "        \"AVProductStatesIdentifier\",\n",
    "        \"CityIdentifier\",\n",
    "        \"GeoNameIdentifier\",\n",
    "        \"OsBuildLab\",\n",
    "        \"IeVerIdentifier\",\n",
    "        \"Census_OEMNameIdentifier\",\n",
    "        \"Census_OEMModelIdentifier\",\n",
    "        \"Census_ProcessorModelIdentifier\",\n",
    "        \"Census_OSVersion\",\n",
    "        \"Census_OSBuildRevision\",\n",
    "        \"Census_FirmwareManufacturerIdentifier\",\n",
    "        \"Census_FirmwareVersionIdentifier\",\n",
    "        \"LocaleEnglishNameIdentifier\"\n",
    "    ]\n",
    "\n",
    "    cols_numerical = [\n",
    "        \"Census_ProcessorCoreCount\",\n",
    "        \"Census_PrimaryDiskTotalCapacity\",\n",
    "        \"Census_SystemVolumeTotalCapacity\",\n",
    "        \"Census_TotalPhysicalRAM\",\n",
    "        \"Census_InternalPrimaryDiagonalDisplaySizeInInches\",\n",
    "        \"Census_InternalPrimaryDisplayResolutionHorizontal\",\n",
    "        \"Census_InternalPrimaryDisplayResolutionVertical\",\n",
    "        \"Census_InternalBatteryNumberOfCharges\"\n",
    "    ]\n",
    "    \n",
    "    # Update our column arrays\n",
    "    cols_categorical = [x for x in cols_categorical if x in cols]\n",
    "    cols_numerical = [x for x in cols_numerical if x in cols]\n",
    "    cols_booleans = [x for x in cols_booleans if x in cols]\n",
    "    cols_categorical_large = [x for x in cols_categorical_large if x in cols]\n",
    "    \n",
    "    return cols_categorical, cols_numerical, cols_booleans, cols_categorical_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing<a id=\"Pre_Processing\"></a>\n",
    "### 1. Mapping appropiate data types\n",
    "In order to understand the quality of the data, we did an extensive review of the data and determined which fields should be considered categorical, continous, or boolean.  We converted 23 id and category fields to object as well as removed outliers to make values boolean.  \n",
    "\n",
    "In order to help provide easier development, we created 4 array of column names: cols_booleans, cols_numerical, cols_categorical, and cols_categorical_large. The values with over 100 possible values were moved to its own bucket, cols_categorical_large, requiring special care to review in order to determine if we could cluster values together.\n",
    "\n",
    "The remaining code are steps we took to clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-184589738fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcols_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_numerical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_booleans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_categorical_large\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_column_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"cols_booleans\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcols_booleans\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"cols_numerical\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "cols_categorical, cols_numerical, cols_booleans, cols_categorical_large = load_column_array\n",
    "\n",
    "print( \"cols_booleans\" ) \n",
    "display( cols_booleans )\n",
    "print( \"cols_numerical\" )\n",
    "display( cols_numerical )\n",
    "print( \"cols_categorical\" )\n",
    "display( cols_categorical )\n",
    "print( \"cols_categorical_large\" )\n",
    "display( cols_categorical_large )\n",
    "\n",
    "#Convert features to right data type\n",
    "malware[cols_categorical] = malware[cols_categorical].astype(object)\n",
    "malware[cols_categorical_large] = malware[cols_categorical_large].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove problomatic columns\n",
    "* Columns with same value in all rows\n",
    "* Columsn with a unique value in all rows\n",
    "* Empty Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueValueCounts = malware.nunique(dropna=False)\n",
    "SingleValueCols = UniqueValueCounts[UniqueValueCounts == 1].index\n",
    "malware = malware.drop(SingleValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print( \"Removing the following columns\\n\", SingleValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with the same value in every row\" )\n",
    "malware.info(verbose=False)\n",
    "print('\\r\\nColumns Deleted: ', len(SingleValueCols) )\n",
    "\n",
    "#Remove any fields that have unique values in every row\n",
    "malwareRecordCt = malware.shape[0]\n",
    "UniqueValueCounts = malware.apply(pd.Series.nunique)\n",
    "AllUniqueValueCols = UniqueValueCounts[UniqueValueCounts == malwareRecordCt].index\n",
    "malware = malware.drop(AllUniqueValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print( \"Removing the following columns\\n\", AllUniqueValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with unique values in every row\" )\n",
    "malware.info(verbose=False)\n",
    "print( \"\\r\\nColumns Deleted: \", len(AllUniqueValueCols) )\n",
    "\n",
    "#Remove any empty fields (null values in every row)\n",
    "malwareRecordCt = malware.shape[0]\n",
    "NullValueCounts = malware.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts == malwareRecordCt].index\n",
    "malware = malware.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print( \"Removing the following columns\\n\", NullValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with null / blank values in every row.\" )\n",
    "malware.info(verbose=False)\n",
    "print( \"\\r\\nColumns Deleted: \", len(NullValueCols) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Columns with low fill-rate or high frequency of same value\n",
    "\n",
    "* Remove columsn with 60% of values are NA or NULL\n",
    "* Remove columsn with 80% of high frequency of same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Eliminate columns with 80% of missing values\n",
    "malwareRecordCt = malware.shape[0]\n",
    "missingValueLimit = malwareRecordCt * .6\n",
    "NullValueCounts = malware.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts >= missingValueLimit].index\n",
    "malware = malware.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print( \"Removing the following columns\\n\", NullValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with >= .6 % of missing values\" )\n",
    "malware.info(verbose=False)\n",
    "print (\"\\r\\nColumns Deleted: \", len(NullValueCols) )\n",
    "\n",
    "#Eliminate values with 80% of the value is the same\n",
    "malwareRecordCt = malware.shape[0]\n",
    "sameValueLimit = malwareRecordCt * .8\n",
    "MaxColumnFreq = malware.apply(lambda x: x.value_counts().values[0])\n",
    "HighFreqCols = MaxColumnFreq[MaxColumnFreq >= sameValueLimit].index\n",
    "malware = malware.drop(HighFreqCols, axis=1)\n",
    "\n",
    "#Review dataset contents after high frequency delete\n",
    "print( \"Removing the following columns\\n\", HighFreqCols )\n",
    "print( \"\\r\\nAfter: Removing columns with >= .8 % of missing values\" )\n",
    "malware.info(verbose=False)\n",
    "print (\"\\r\\nColumns Deleted: \", len(HighFreqCols) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our column arrays\n",
    "cols_categorical = [x for x in cols_categorical if x in malware.columns]\n",
    "cols_numerical = [x for x in cols_numerical if x in malware.columns]\n",
    "cols_booleans = [x for x in cols_booleans if x in malware.columns]\n",
    "cols_categorical_large = [x for x in cols_categorical_large if x in malware.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Impute Missing Values\n",
    "#### Census Hardware Configurations\n",
    "The continuous values represent all of hardware configurations on a machine, for example memory and hard drive capacity.  In order to assign the right value, we will use the median value grouped by **Census_MDC2FormFactor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the median value for each field\n",
    "rowsBefore = malware[cols_numerical].isnull().T.any().T.sum()\n",
    "\n",
    "#Removing values less then 0\n",
    "malware[cols_numerical] = malware[cols_numerical].replace(-1, np.nan)\n",
    "\n",
    "malware[cols_numerical] = malware.groupby(\"Census_MDC2FormFactor\")[cols_numerical]\\\n",
    "    .transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "#Review dataset contents after Census_MDC2FormFactor Census hardware Imputation\n",
    "print(\"After: Updating Missing Continous Values\")   \n",
    "rowsAfter = malware[cols_numerical].isnull().T.any().T.sum()\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows Updated / Imputed: ', rowsUpdated )\n",
    "print('\\r\\nTotal Rows Missing Census Hardware by Census_MDC2FormFactor') \n",
    "malware['Census_MDC2FormFactor'][malware[cols_numerical].isnull().T.any().T].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Census Categorial Configurations\n",
    "Assign remaining missing values associated to Census details based on the mode of **Census_MDC2FormFactor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "('index out of bounds', 'occurred at index Census_ProcessorClass')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2559\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2560\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8e9a25be91d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Group by FormFactor and OSSkuName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNullValueCols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Census_MDC2FormFactor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNullValueCols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Review dataset contents after Census Mode Imputations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m         \u001b[0;31m# a reduction transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m_transform_general\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4027\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4028\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4029\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4031\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mfast_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m             slow_path = lambda group: group.apply(\n\u001b[0;32m-> 4113\u001b[0;31m                 lambda x: func(x, *args, **kwargs), axis=self.axis)\n\u001b[0m\u001b[1;32m   4114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslow_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4875\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4876\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4877\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4878\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4879\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4971\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4972\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4973\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4974\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4975\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mfast_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m             slow_path = lambda group: group.apply(\n\u001b[0;32m-> 4113\u001b[0;31m                 lambda x: func(x, *args, **kwargs), axis=self.axis)\n\u001b[0m\u001b[1;32m   4114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslow_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8e9a25be91d3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Group by FormFactor and OSSkuName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNullValueCols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Census_MDC2FormFactor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNullValueCols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Review dataset contents after Census Mode Imputations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2566\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2567\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: ('index out of bounds', 'occurred at index Census_ProcessorClass')"
     ]
    }
   ],
   "source": [
    "#Get all census fields\n",
    "CensusFields = malware.filter(regex='Census').columns\n",
    "NullValueCounts = malware[CensusFields].isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts > 0 ].index\n",
    "\n",
    "rowsBefore = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "\n",
    "#Group by FormFactor and OSSkuName\n",
    "malware[NullValueCols] = malware.groupby([\"Census_MDC2FormFactor\"])[NullValueCols]\\\n",
    "    .transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "#Review dataset contents after Census Mode Imputations\n",
    "print(\"After: Updating Missing Continous Values\")   \n",
    "rowsAfter = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows Updated / Imputed: ', rowsUpdated )\n",
    "print('\\r\\nTotal Rows Missing for Census Fields: ',malware[CensusFields].isnull().T.any().T.sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remaining Features\n",
    "Set the remaining features as 0, to indicate not turned on (boolean) or feature available (categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NullValueCounts = malware.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts > 0 ].index\n",
    "\n",
    "rowsBefore = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "\n",
    "malware[NullValueCols] = malware[NullValueCols].fillna(0)\n",
    "\n",
    "#Review dataset contents after Census Mode Imputations\n",
    "print(\"After: Updating Missing Values\")   \n",
    "rowsAfter = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows Updated / Imputed: ', rowsUpdated )\n",
    "print('\\r\\nTotal Rows Missing values: ',malware.isnull().T.any().T.sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart Screen fill miising values and fix characters issue\n",
    "malware.SmartScreen.replace({\"off\":\"Off\",\"00000000\":\"ExistsNotSet\",\"&#x02;\" :\"ExistsNotSet\",\n",
    "                                 \"&#x01;\" :\"ExistsNotSet\",\"0\":\"ExistsNotSet\"},inplace=True)\n",
    "# currently renamed \"Census_PrimaryDiskTypeName\" unknown data into one category\n",
    "malware.Census_PrimaryDiskTypeName.replace({\"Unspecified\":\"Other\"},inplace=True)\n",
    "\n",
    "# currently renamed \"Census_ChassisTypeName\" unknown data into one category\n",
    "malware.Census_ChassisTypeName.replace({\"UNKNOWN\":\"Other\",\"Unknown\":\"Other\",\"0\" :\"Other\",\n",
    "                                \"30\" :\"Other\",\n",
    "                                \"35\" :\"Other\",\n",
    "                                \"112\" :\"Other\",\n",
    "                                \"76\" :\"Other\",\n",
    "                                \"39\" :\"Other\"},inplace=True)\n",
    "\n",
    "# currently renamed \"Census_PowerPlatformRoleName\" unknown data into one category\n",
    "\n",
    "malware.Census_PowerPlatformRoleName.fillna('Other', inplace=True)\n",
    "\n",
    "malware.Census_PowerPlatformRoleName.replace({\"UNKNOWN\":\"Other\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: Deleting rows\n",
      "Rows removed:  0\n"
     ]
    }
   ],
   "source": [
    "rowsBefore = len(malware)\n",
    "malware = malware.drop_duplicates()\n",
    "\n",
    "\n",
    "#Review dataset after deleting duplicates\n",
    "print(\"After: Deleting rows\")   \n",
    "rowsAfter = len(malware)\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows removed: ', rowsUpdated )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. New Features\n",
    "**AvSigVersion** is a version number with its format of *1.217.1014.0*.  To reduce our columnes for One-Hot Encoding, we reduced the version to just Major.Minor.  or *1.217*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for AvSigVersion is  5704\n",
      "After the transofmration of Major.Minor, the unique features reudced to  36\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cols_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-aa8a7ce47c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Add AvSigVersion_x_x to cols_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcols_categorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AvSigVersion_x_x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Add to data glossary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cols_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "AvSigUniqueValueCounts = malware[\"AvSigVersion\"].nunique()\n",
    "\n",
    "AvSigVersion_split = malware[\"AvSigVersion\"].str.rsplit(pat=\".\",expand=True)\n",
    "malware[\"AvSigVersion_x_x\"] = AvSigVersion_split.loc[:,0]+\".\"+AvSigVersion_split.loc[:,1]\n",
    "\n",
    "AvSigXXUniqueValueCounts = malware[\"AvSigVersion_x_x\"].nunique()\n",
    "\n",
    "print( \"The unique values for AvSigVersion is \", AvSigUniqueValueCounts)\n",
    "print( \"After the transofmration of Major.Minor, the unique features reudced to \", AvSigXXUniqueValueCounts)\n",
    "\n",
    "# Add AvSigVersion_x_x to cols_categorical\n",
    "cols_categorical.append(\"AvSigVersion_x_x\")\n",
    "\n",
    "# Add to data glossary\n",
    "data_glossary = data_glossary.append(\n",
    "    pd.DataFrame(\n",
    "        data = [\"New Feature: Reduced version of Defender state information e.g. 1.217\"],\n",
    "        index = [\"AvSigVersion_x_x\"],\n",
    "        columns = [\"description\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our dataset as malware.clean.csv\n",
    "malware.to_csv(\"data/malware.clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection \\ Model Building<a id=\"Feature_Selection_Modeling_Building\"></a>\n",
    "Following functions are used during model building\n",
    "* Create Hot=One Encoding\n",
    "* Remove Highly Corrleated Features\n",
    "\n",
    "During our modeling, we will explore with scalling the data or PCA for feature reductions. **CHECK WITH DR. DREW WHY THIS IS REQUIRED IN DATA PREP????**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encodings(df, cols):\n",
    "    result = pd.DataFrame()\n",
    "    i = 0\n",
    "    for col in cols:\n",
    "        dummies = pd.get_dummies(df[col],prefix=col)\n",
    "        if( i == 0 ):\n",
    "            result = dummies.copy()\n",
    "        else:\n",
    "            result = pd.concat((result, dummies), axis=1)\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduce_features(df, verbose = False):\n",
    "    # calculate the correlation matrix\n",
    "    corr_matrix  = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    \n",
    "    #Get all of the correlation values > 95%\n",
    "    x = np.where(upper > 0.95)\n",
    "\n",
    "    #Display all field combinations with > 95% correlation\n",
    "    cf = pd.DataFrame()\n",
    "    cf['Field1'] = upper.columns[x[1]]\n",
    "    cf['Field2'] = upper.index[x[0]]\n",
    "\n",
    "    #Get the correlation values for every field combination. (There must be a more pythonic way to do this!)\n",
    "    corr = [0] * len(cf)\n",
    "    for i in range(0, len(cf)):\n",
    "        corr[i] =  upper[cf['Field1'][i]][cf['Field2'][i]] \n",
    "\n",
    "    cf['Correlation'] = corr\n",
    "\n",
    "    if( verbose ):\n",
    "        print('There are ', str(len(cf['Field1'])), ' field correlations > 95%.')\n",
    "        display(cf)\n",
    "        \n",
    "        print('Dropping the following ', str(len(to_drop)), ' highly correlated fields.')\n",
    "        to_drop\n",
    "        \n",
    "    #Check columns before drop \n",
    "    if( verbose ):\n",
    "        print('\\r\\n*********Before: Dropping Highly Correlated Fields*************************************')\n",
    "        display(df.info(verbose=False))\n",
    "\n",
    "    # Drop the highly correlated features from our training data \n",
    "    df = df.drop(to_drop, axis=1)\n",
    "\n",
    "    #Check columns after drop \n",
    "    if( verbose ):\n",
    "        print('\\r\\n*********After: Dropping Highly Correlated Fields**************************************')\n",
    "        df.info(verbose=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of feature reductions on highly corrleated values\n",
    "1. Examine the columns\n",
    "    * We will create dummy variables for all columns that have less then 100 unique values, which are classified in the cols_categorical array.\n",
    "2. Remove highly correlated values\n",
    "\n",
    "During our modeling building we will include the larger categorical variables as needed in order to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbourzik/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cols_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-44361de0eac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmalware\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/malware.clean.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical_large\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical_large\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moneHotUniqueValueCounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cols_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "malware = pd.read_csv(\"data/malware.clean.csv\")\n",
    "malware[cols_categorical] = malware[cols_categorical].astype(object)\n",
    "malware[cols_categorical_large] = malware[cols_categorical_large].astype(object)\n",
    "\n",
    "oneHotUniqueValueCounts = malware[cols_categorical].apply(lambda x: x.nunique())\n",
    "print( \"Cateogiries that have less then 100 unique values\" )\n",
    "display(oneHotUniqueValueCounts)\n",
    "\n",
    "oneHotUniqueValueCounts = malware[cols_categorical_large].apply(lambda x: x.nunique())\n",
    "print( \"Cateogiries that have more then 50 unique values\" )\n",
    "display(oneHotUniqueValueCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.concat(\n",
    "    (    \n",
    "        malware[cols_booleans],\n",
    "        malware[cols_numerical],\n",
    "        get_one_hot_encodings(malware, cols_categorical)\n",
    "    ), axis = 1)\n",
    "\n",
    "print(\"Total Features after One-Hot Encoding: \", model_data.shape )\n",
    "print(\"Reduce Features based on highly correlated values > .95\")\n",
    "model_data = reduce_features(model_data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definitions<a id=\"Data_Definitions\"></a>\n",
    "\n",
    "The following values are the remaining fields we will use after removing columns we found problamatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 120)\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "# Only display fields remaining in malware\n",
    "display( data_glossary.loc[malware.columns,:] )\n",
    "\n",
    "pd.reset_option('max_colwidth')\n",
    "pd.reset_option('max_row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation <a id=\"Modeling_and_Evaluation\"></a>\n",
    "1. [Evaluation Metrics](#Evaluation_Metrics)\n",
    "2. [Training Methods](#Training_Methods)\n",
    "3. [Classification Models](#Classification_Models)\n",
    "    * [Support Vector Machine](#Support_Vector_Machine)\n",
    "    * [Random Forest](#Random_Forest)\n",
    "    * [K-Nearest Neighbors](#K_Nearest_Neighbors)\n",
    "4. Model Compersion\n",
    "5. Model Advantages\n",
    "6. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics<a id=\"Evaluation_Metrics\"></a>\n",
    "\n",
    "Our response **HasDections** has a binary value of 1 or 0, 1 indicating a machine has been attacked by malware.  We will use the data presented to us as a single point in time of the attack to determine if we can predict future malware attacks.  In order to evaluate each of our models, we will use the following techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "<table border=1 cellspacing=0 cellpadding=0 >\n",
    "  <tr>\n",
    "    <td width=198 colspan=2 rowspan=2>&nbsp;</td>\n",
    "    <td width=426 colspan=2 valign=center>\n",
    "        <p><h3>Predictions</h3></p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr >\n",
    "    <td width=216>\n",
    "        <p><b>No</b></p>\n",
    "    </td>\n",
    "    <td width=210>\n",
    "      <p><b>Yes</b></p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td width=78 rowspan=2>\n",
    "      <h3>Actual</h3>\n",
    "    </td>\n",
    "    <td width=120 p>\n",
    "      <p><b>No</b></p>\n",
    "    </td>\n",
    "    <td width=216>\n",
    "      <p><b>True Negative</b> - Successfully identifies machines that are not attacked by Malware</p>\n",
    "    </td>\n",
    "    <td width=210>\n",
    "        <p><b>False Postive </b> - These machines were predicted to be under attack, but these truely are not.  We will use precision to help measurement.  Higher the precision, the less machines are prone to future attacks.</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td width=120>\n",
    "      <p><b>Yes</b></p>\n",
    "    </td>\n",
    "    <td width=216>\n",
    "      <p><b>False Negative</b> - These machines were attacked, however we predicted they did not have the characteristics of being vulnerable.  Many scenarios could be made as to why we failed to predict a single machine.  In order to determine the best model, we adjust our probabilities to provide us the lowest recall value.  </p> \n",
    "    </td>\n",
    "    <td width=210>\n",
    "      <p><b>True Postive</b> - Successfully identifies machines that are attacked by Malware</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From GB\n",
    "\n",
    "The first step in determining what metrics should be used, is to outline the challenge the data will present.  Since this is a binary classification problem, each model will be assessed to determin the eficacy the models.  There are three key types of metrics that will be used: ROC Curves, Confusion Matrices, and statistcs:\n",
    "\n",
    "* <u>ROC Curves</u> - Each model that is assessed will have a a grpahical represntation of the ROC curve and AUC.  Additionally, all models will be aggregated and presented individually to compare against each other.\n",
    "* <u>Confusion Matrices</u> - Each model will have a final confusion matrix that will be presented.  While we will be concerned with the accuracy of True Postivies and True Negatives, Type 1 error (False Postives) and Type 2 error (False Negatives) will be analyzed.  Type 2 Error, False Negatives, are systems that did get compromised that were missed in the model. While Type 1 error of Fales Postives are important, this can often led to a resource waste in attempting to remedate malware in which no malware is presented. Additionaly, precision, recall, and the F1 Measurement will be analzyed.  Because there will be weak learners, boosting will be used to ensure the difficult learners will be captured.\n",
    "* <u>Model Comparision</u> As models are analzyed, we will compare the models to determine the differences between inccorrect and correct predictions between Model 1 and Model 2.  This is a very similiar matrix as the confusion matrix, but this will allow a model comparision.  The goals is to have a great number in the statistics that Model 1 and 2 are correct  and incoorect together as compared to discrepncy between both model predictions.\n",
    "* <u>Statistifcs</u> Each model will additinaly analzyed to determine RMSE, R2, and MSE as an additional check and validation of the ROC Curves and Confusion Matrices\n",
    "* <u>Clustering</u> - For each cluster, the team will review elbow charts to determine the optimal number of clusters for k-means clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From GB - Probably should remove this\n",
    "### ROC Curve\n",
    "We will use the ROC Curve as an visual aid to determine if the probabilities values we predicted can be adjusted in order to improve our accuracy, while keeping our Recal low.  A secondary measurment will be the area under the curve, which will help us capture the overall performance of each model instead of just accuracy.\n",
    "\n",
    "### Overall Accuracy\n",
    "Accuracy is still a key in\n",
    "all predictions and we will leverage it for our model in order to measure its performance.\n",
    "\n",
    "### Root Mean Square Error\n",
    "To determine how well our model does are predictions, we will measure the Root Mean Square Error.  The RMSE will measure the different between Actual and Predicted.  The lower the value, the better our model performed with less variance.\n",
    "\n",
    "### Model Compersion\n",
    "In order to compare each of our models, we will use the above statitics and perform a t-test on its results (either 5 or 10 folds of results).  We will be able to statistically tell you which model performed better or the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Methods<a id=\"Training_Methods\"></a>\n",
    "\n",
    "Our original data set has over 300k rows and 375 to 8,000 columns after data preperation and one-hot encoding.  Given our size of the data, we are confident that our model will not be underfitted with any training method choosen.  However, we do have concern regarding overfitting our model.  Another issue with the size of our data set is the system computation it takes to create our model.  \n",
    "\n",
    "In order to address overfitting our model and create a model within a managable amount of time, will will use the following technique.  We have a balance class of **HasDetections**, so our sampling techniques will be a simple randomizer.\n",
    "1. Sub sample 20% of our data.\n",
    "2. 10-Fold Cross Validation with a 90/10 split for training and testing\n",
    "\n",
    "\n",
    "#### Variables\n",
    "* `subMalware`: Sub Sample 20% of the original malware data frame\n",
    "* `cv_object`: Cross Validation Object used for Training our Model\n",
    "\n",
    "*For reproducible research, we will use `random.seed` paramter with a value of <b>42</b>*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our subsample data has the following number of rows:  66682\n",
      "Cross Validation object test size of 20% with 5 iterations\n",
      "\n",
      "ShuffleSplit(n_splits=10, random_state=42, test_size=0.1, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Create a Sub Sample of our malware data\n",
    "rowCounts = len(malware)\n",
    "k = .20\n",
    "\n",
    "np.random.seed(42)\n",
    "finalSampleIndex = np.random.choice(rowCounts, int(rowCounts*k), replace=False)\n",
    "\n",
    "subMalware = malware.iloc[finalSampleIndex,:].reset_index()\n",
    "del subMalware['index']\n",
    "\n",
    "print( \"Our subsample data has the following number of rows: \", len(subMalware) )\n",
    "\n",
    "#Save a Sub Sample\n",
    "subMalware.to_csv(\"data/malware.subsample.csv\")\n",
    "\n",
    "# Extrac 20% for a test set\n",
    "# rowCounts = len(subMalware)\n",
    "# k = .2\n",
    "\n",
    "# np.random.seed(42)\n",
    "# testSampleIndex = np.random.choice(rowCounts, int(rowCounts*k), replace=False)\n",
    "# trainMalware = subMalware.drop(testSampleIndex)\n",
    "# testMalware = subMalware.iloc[testSampleIndex,:]\n",
    "\n",
    "# print( \"Training\\Validation Dataset Row Count: \", len(trainMalware) )\n",
    "# print( \"Test Dataset Row Count: \", len(testMalware) )\n",
    "\n",
    "# Creat eth Cross Validation Objected used for all tests\n",
    "num_cv_iterations = 10\n",
    "cv_object = ShuffleSplit(\n",
    "    n_splits=num_cv_iterations,\n",
    "    test_size  = 0.1,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "print( \"Cross Validation object test size of 20% with 5 iterations\\n\" )\n",
    "print( cv_object )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Models<a id=\"Classification_Models\"></a>\n",
    "In order to determine the best model, we will reviewing the following Machine Learning techniques\n",
    "* Support Vector Machine\n",
    "* Random Forest\n",
    "* K-Nearest Neighbors\n",
    "\n",
    "During our model building, we will review feature importance and parameter adjustments in order to create the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is GB's Attempt at RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols_booleans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cols_booleans' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "X=malware[cols_booleans].values\n",
    "y=malware[\"HasDetections\"].values\n",
    "\n",
    "scl = StandardScaler()\n",
    "X = scl.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
