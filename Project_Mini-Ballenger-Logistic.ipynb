{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 7331 Data Mining\n",
    "### Logistic Regression and SVM\n",
    "### Mini Lab\n",
    "* Tahir Ahmad<br>\n",
    "* Christopher Ballenger<br>\n",
    "* Grant Bourzikas<br>\n",
    "* Vitaly Briker<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "%run -i ColumnArrays.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 390 ms, total: 4.73 s\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%time final = pd.read_csv(\"data/clean.final.csv\")\n",
    "\n",
    "final[cols_categorical] = final[cols_categorical].astype(object)\n",
    "final[cols_categorical_large] = final[cols_categorical_large].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables\n",
    "Created Dummy Variables for all the cols_categorical variables\n",
    "How to use it.\n",
    "\n",
    "Merge any Response with a variable\n",
    "```python\n",
    "df = pd.concat((Response,EngineVersion, Census_MDC2FormFactor),axis=1)\n",
    "```\n",
    "**Do not sort result or reset index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333411, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Response = final[[\"HasDetections\", \"MachineIdentifier\"]]\n",
    "display(Response.shape)\n",
    "\n",
    "scl = StandardScaler()\n",
    "\n",
    "scl_numericals = scl.fit_transform(final[cols_numerical])\n",
    "scl_numericals = pd.DataFrame(data=scl_numericals,\n",
    "    columns=cols_numerical)  \n",
    "\n",
    "EngineVersion = pd.get_dummies(final[\"EngineVersion\"],prefix=\"EngineVersion\")\n",
    "RtpStateBitfield = pd.get_dummies(final[\"RtpStateBitfield\"],prefix=\"RtpStateBitfield\")\n",
    "AVProductsInstalled = pd.get_dummies(final[\"AVProductsInstalled\"],prefix=\"AVProductsInstalled\")\n",
    "AVProductsEnabled = pd.get_dummies(final[\"AVProductsEnabled\"],prefix=\"AVProductsEnabled\")\n",
    "OrganizationIdentifier = pd.get_dummies(final[\"OrganizationIdentifier\"],prefix=\"OrganizationIdentifier\")\n",
    "Platform = pd.get_dummies(final[\"Platform\"],prefix=\"Platform\")\n",
    "Processor = pd.get_dummies(final[\"Processor\"],prefix=\"Processor\")\n",
    "OsVer = pd.get_dummies(final[\"OsVer\"],prefix=\"OsVer\")\n",
    "OsBuild = pd.get_dummies(final[\"OsBuild\"],prefix=\"OsBuild\")\n",
    "OsSuite = pd.get_dummies(final[\"OsSuite\"],prefix=\"OsSuite\")\n",
    "OsPlatformSubRelease = pd.get_dummies(final[\"OsPlatformSubRelease\"],prefix=\"OsPlatformSubRelease\")\n",
    "SkuEdition = pd.get_dummies(final[\"SkuEdition\"],prefix=\"SkuEdition\")\n",
    "SmartScreen = pd.get_dummies(final[\"SmartScreen\"],prefix=\"SmartScreen\")\n",
    "Census_MDC2FormFactor = pd.get_dummies(final[\"Census_MDC2FormFactor\"],prefix=\"Census_MDC2FormFactor\")\n",
    "Census_ProcessorManufacturerIdentifier = pd.get_dummies(final[\"Census_ProcessorManufacturerIdentifier\"],prefix=\"Census_ProcessorManufacturerIdentifier\")\n",
    "Census_PrimaryDiskTypeName = pd.get_dummies(final[\"Census_PrimaryDiskTypeName\"],prefix=\"Census_PrimaryDiskTypeName\")\n",
    "Census_ChassisTypeName = pd.get_dummies(final[\"Census_ChassisTypeName\"],prefix=\"Census_ChassisTypeName\")\n",
    "Census_PowerPlatformRoleName = pd.get_dummies(final[\"Census_PowerPlatformRoleName\"],prefix=\"Census_PowerPlatformRoleName\")\n",
    "Census_OSArchitecture = pd.get_dummies(final[\"Census_OSArchitecture\"],prefix=\"Census_OSArchitecture\")\n",
    "Census_OSBranch = pd.get_dummies(final[\"Census_OSBranch\"],prefix=\"Census_OSBranch\")\n",
    "Census_OSBuildNumber = pd.get_dummies(final[\"Census_OSBuildNumber\"],prefix=\"Census_OSBuildNumber\")\n",
    "Census_OSEdition = pd.get_dummies(final[\"Census_OSEdition\"],prefix=\"Census_OSEdition\")\n",
    "Census_OSSkuName = pd.get_dummies(final[\"Census_OSSkuName\"],prefix=\"Census_OSSkuName\")\n",
    "Census_OSInstallTypeName = pd.get_dummies(final[\"Census_OSInstallTypeName\"],prefix=\"Census_OSInstallTypeName\")\n",
    "Census_OSInstallLanguageIdentifier = pd.get_dummies(final[\"Census_OSInstallLanguageIdentifier\"],prefix=\"Census_OSInstallLanguageIdentifier\")\n",
    "Census_OSUILocaleIdentifier = pd.get_dummies(final[\"Census_OSUILocaleIdentifier\"],prefix=\"Census_OSUILocaleIdentifier\")\n",
    "Census_OSWUAutoUpdateOptionsName = pd.get_dummies(final[\"Census_OSWUAutoUpdateOptionsName\"],prefix=\"Census_OSWUAutoUpdateOptionsName\")\n",
    "Census_GenuineStateName = pd.get_dummies(final[\"Census_GenuineStateName\"],prefix=\"Census_GenuineStateName\")\n",
    "Census_ActivationChannel = pd.get_dummies(final[\"Census_ActivationChannel\"],prefix=\"Census_ActivationChannel\")\n",
    "Census_FlightRing = pd.get_dummies(final[\"Census_FlightRing\"],prefix=\"Census_FlightRing\")\n",
    "Wdft_RegionIdentifier = pd.get_dummies(final[\"Wdft_RegionIdentifier\"],prefix=\"Wdft_RegionIdentifier\")\n",
    "\n",
    "AppVersion = pd.get_dummies(final[\"AppVersion\"],prefix=\"AppVersion\")\n",
    "AvSigVersion = pd.get_dummies(final[\"AvSigVersion\"],prefix=\"AvSigVersion\")\n",
    "AVProductStatesIdentifier  = pd.get_dummies(final[\"AVProductStatesIdentifier\"],prefix=\"AVProductStatesIdentifier\")\n",
    "\n",
    "AppVersion_split = final[\"AppVersion\"].str.rsplit(pat=\".\",expand=True)\n",
    "final[\"AppVersion_x_x\"] = AppVersion_split.loc[:,0]+\".\"+AppVersion_split.loc[:,1]#+\".\"+AppVersion_split.loc[:,2]\n",
    "\n",
    "AvSigVersion_split = final[\"AvSigVersion\"].str.rsplit(pat=\".\",expand=True)\n",
    "final[\"AvSigVersion_x_x\"] = AvSigVersion_split.loc[:,0]+\".\"+AvSigVersion_split.loc[:,1]\n",
    "final[\"AvSigVersion_x_x_x\"] = AvSigVersion_split.loc[:,0]+\".\"+AvSigVersion_split.loc[:,1]+\".\"+AvSigVersion_split.loc[:,2].str[:2]\n",
    "\n",
    "AppVersion_x_x = pd.get_dummies(final[\"AppVersion_x_x\"],prefix=\"AppVersion_x_x\")\n",
    "AvSigVersion_x_x = pd.get_dummies(final[\"AvSigVersion_x_x\"],prefix=\"AvSigVersion_x_x\")\n",
    "AvSigVersion_x_x_x  = pd.get_dummies(final[\"AvSigVersion_x_x_x\"],prefix=\"AvSigVersion_x_x_x\")\n",
    "\n",
    "\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((Response,EngineVersion, Census_MDC2FormFactor),axis=1)\n",
    "\n",
    "# Validate a couple\n",
    "display(final.loc[final.MachineIdentifier.isin([\n",
    "    \"000046e59c37136173428e560acbe3a2\",\n",
    "    \"0000d738ef47f088c53fef6013268ac1\",\n",
    "    \"ffff192b25a721196450283732616ef8\",\n",
    "    \"ffff4e2671cb933424fdf6f6b237cce3\"]),\n",
    "    [\"MachineIdentifier\",\"HasDetections\",\"EngineVersion\",\"Census_MDC2FormFactor\"]])\n",
    "\n",
    "display(df.loc[final.MachineIdentifier.isin([\n",
    "    \"000046e59c37136173428e560acbe3a2\",\n",
    "    \"0000d738ef47f088c53fef6013268ac1\",\n",
    "    \"ffff192b25a721196450283732616ef8\",\n",
    "    \"ffff4e2671cb933424fdf6f6b237cce3\"]),:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Feature Selections\n",
    "We want to learn what defender characteristics are factoring in to the HasDetection values (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3085,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['IsProtected', 'Firewall', 'AVProductStatesIdentifier_20.0', ...,\n",
       "       'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
       "       'Census_InternalPrimaryDisplayResolutionVertical',\n",
       "       'Census_InternalBatteryNumberOfCharges'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols = [\"IsProtected\",\"firewall\"]\n",
    "\n",
    "df = pd.concat(\n",
    "    (\n",
    "        final[[\"IsProtected\",\"Firewall\"]],\n",
    "        AVProductStatesIdentifier,\n",
    "        AVProductsInstalled,\n",
    "        AVProductsEnabled,\n",
    "        SmartScreen,\n",
    "        AppVersion_x_x,\n",
    "        EngineVersion,\n",
    "        AvSigVersion_x_x\n",
    "    ),axis=1)\n",
    "\n",
    "x_labels = np.append(df.columns.values, cols_numerical)\n",
    "display(x_labels.shape)\n",
    "x_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = df.corr()\n",
    "# df_heatmap, ax = plt.subplots(figsize=(40,40))\n",
    "# sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "#             square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adjusting for similar variables, there are some highly correlated values, but all are represneting something different in our data set.  Features that are highly correlated show characteristics of being part of a hardware family, for example a specific Core Manufacture Identifier is highly correlated to Tablets.\n",
    "\n",
    "### Full Model\n",
    "To determine if overfitting occurs, we will use a CV with 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "y = Response['HasDetections'].values\n",
    "#Identify features not related to specific builds\n",
    "X = df.values\n",
    "\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.48 s, sys: 2.98 s, total: 12.5 s\n",
      "Wall time: 12.6 s\n",
      "====Iteration 0  ====\n",
      "accuracy 0.6032422056596134\n",
      "confusion matrix\n",
      " [[18221 14009]\n",
      " [12448 22005]]\n",
      "total number of paramters 2754\n",
      "CPU times: user 10.4 s, sys: 3.04 s, total: 13.4 s\n",
      "Wall time: 13.5 s\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6030622497488115\n",
      "confusion matrix\n",
      " [[18049 13917]\n",
      " [12552 22165]]\n",
      "total number of paramters 2729\n",
      "CPU times: user 9.7 s, sys: 3.02 s, total: 12.7 s\n",
      "Wall time: 12.8 s\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6045168933611266\n",
      "confusion matrix\n",
      " [[18574 13507]\n",
      " [12865 21737]]\n",
      "total number of paramters 2720\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    %time lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    auc = mt.roc_auc_score(y_test, y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    print(\"area under the curve\\n\",auc)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from our model regarding features related to Defender is about 60% and an AUC of around XX.  To improve our model we will include details about the machine to help learn what type of user would be vulerenable to malware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census_ProcessorCoreCount</th>\n",
       "      <th>Census_PrimaryDiskTotalCapacity</th>\n",
       "      <th>Census_SystemVolumeTotalCapacity</th>\n",
       "      <th>Census_TotalPhysicalRAM</th>\n",
       "      <th>Census_InternalPrimaryDiagonalDisplaySizeInInches</th>\n",
       "      <th>Census_InternalPrimaryDisplayResolutionHorizontal</th>\n",
       "      <th>Census_InternalPrimaryDisplayResolutionVertical</th>\n",
       "      <th>Census_InternalBatteryNumberOfCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.762305</td>\n",
       "      <td>-0.264274</td>\n",
       "      <td>-0.244221</td>\n",
       "      <td>-0.834531</td>\n",
       "      <td>-0.394881</td>\n",
       "      <td>-0.280279</td>\n",
       "      <td>-0.411864</td>\n",
       "      <td>-0.578534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.513001</td>\n",
       "      <td>0.421715</td>\n",
       "      <td>-0.725480</td>\n",
       "      <td>-0.303758</td>\n",
       "      <td>-0.374736</td>\n",
       "      <td>-0.280279</td>\n",
       "      <td>-0.411864</td>\n",
       "      <td>-0.578534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.762305</td>\n",
       "      <td>-0.758189</td>\n",
       "      <td>-1.092128</td>\n",
       "      <td>-0.303758</td>\n",
       "      <td>-0.072571</td>\n",
       "      <td>-0.280279</td>\n",
       "      <td>-0.411864</td>\n",
       "      <td>-0.578534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.762305</td>\n",
       "      <td>1.107707</td>\n",
       "      <td>1.233269</td>\n",
       "      <td>-0.303758</td>\n",
       "      <td>-0.394881</td>\n",
       "      <td>-0.304267</td>\n",
       "      <td>-0.411864</td>\n",
       "      <td>-0.578534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.513001</td>\n",
       "      <td>1.107707</td>\n",
       "      <td>1.170797</td>\n",
       "      <td>-0.303758</td>\n",
       "      <td>-0.072571</td>\n",
       "      <td>-0.280279</td>\n",
       "      <td>-0.411864</td>\n",
       "      <td>-0.578534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Census_ProcessorCoreCount  Census_PrimaryDiskTotalCapacity  \\\n",
       "0                  -0.762305                        -0.264274   \n",
       "1                   0.513001                         0.421715   \n",
       "2                  -0.762305                        -0.758189   \n",
       "3                  -0.762305                         1.107707   \n",
       "4                   0.513001                         1.107707   \n",
       "\n",
       "   Census_SystemVolumeTotalCapacity  Census_TotalPhysicalRAM  \\\n",
       "0                         -0.244221                -0.834531   \n",
       "1                         -0.725480                -0.303758   \n",
       "2                         -1.092128                -0.303758   \n",
       "3                          1.233269                -0.303758   \n",
       "4                          1.170797                -0.303758   \n",
       "\n",
       "   Census_InternalPrimaryDiagonalDisplaySizeInInches  \\\n",
       "0                                          -0.394881   \n",
       "1                                          -0.374736   \n",
       "2                                          -0.072571   \n",
       "3                                          -0.394881   \n",
       "4                                          -0.072571   \n",
       "\n",
       "   Census_InternalPrimaryDisplayResolutionHorizontal  \\\n",
       "0                                          -0.280279   \n",
       "1                                          -0.280279   \n",
       "2                                          -0.280279   \n",
       "3                                          -0.304267   \n",
       "4                                          -0.280279   \n",
       "\n",
       "   Census_InternalPrimaryDisplayResolutionVertical  \\\n",
       "0                                        -0.411864   \n",
       "1                                        -0.411864   \n",
       "2                                        -0.411864   \n",
       "3                                        -0.411864   \n",
       "4                                        -0.411864   \n",
       "\n",
       "   Census_InternalBatteryNumberOfCharges  \n",
       "0                              -0.578534  \n",
       "1                              -0.578534  \n",
       "2                              -0.578534  \n",
       "3                              -0.578534  \n",
       "4                              -0.578534  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(scl_numericals.head())\n",
    "\n",
    "df1 = pd.concat(\n",
    "    (\n",
    "        df,\n",
    "        scl_numericals,\n",
    "        Census_MDC2FormFactor,\n",
    "        Census_OSSkuName,\n",
    "        Census_GenuineStateName,\n",
    "        Census_ActivationChannel\n",
    "    ),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "y = Response['HasDetections'].values\n",
    "#Identify features not related to specific builds\n",
    "X = df1.values\n",
    "\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 3.23 s, total: 26.6 s\n",
      "Wall time: 26.8 s\n",
      "CPU times: user 938 ms, sys: 751 ms, total: 1.69 s\n",
      "Wall time: 1.58 s\n",
      "====Iteration 0  ====\n",
      "accuracy 0.6217626681463042\n",
      "confusion matrix\n",
      " [[17080 14940]\n",
      " [10282 24381]]\n",
      "area under the curve\n",
      " 0.6183945433521295\n",
      "CPU times: user 25.4 s, sys: 3.13 s, total: 28.5 s\n",
      "Wall time: 28.7 s\n",
      "CPU times: user 945 ms, sys: 763 ms, total: 1.71 s\n",
      "Wall time: 1.6 s\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6167239026438522\n",
      "confusion matrix\n",
      " [[16921 15101]\n",
      " [10457 24204]]\n",
      "area under the curve\n",
      " 0.613362208295132\n",
      "CPU times: user 24.7 s, sys: 3.19 s, total: 27.8 s\n",
      "Wall time: 28.1 s\n",
      "CPU times: user 937 ms, sys: 733 ms, total: 1.67 s\n",
      "Wall time: 1.55 s\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6194832266094806\n",
      "confusion matrix\n",
      " [[17071 14844]\n",
      " [10530 24238]]\n",
      "area under the curve\n",
      " 0.616012423596416\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    %time lr_clf.fit(X_train,y_train)  # train object\n",
    "    %time y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    auc = mt.roc_auc_score(y_test, y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    print(\"area under the curve\\n\",auc)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,x_labels) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By scaling the values, we are able to improve our score by 10% to 61%.  Since we are only looking at a subset of all our values, we really want to understand what variables are helping us and which ones provide no value (p-value < 0.05).\n",
    "\n",
    "Let's review our plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df.columns)\n",
    "weights.plot(kind='bar',figsize=(40,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {\n",
    "    'x': df.columns,\n",
    "    'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "    'type': 'bar'\n",
    "}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
