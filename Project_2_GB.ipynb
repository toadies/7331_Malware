{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubic\n",
    "Grading Rubric\n",
    "* **[Data Preparation](#Data_Preperation)** (15 points total)\n",
    "    * **[10 points]** Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "    * **[5 points]** Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "* **[Modeling and Evaluation](#Modeling_and_Evaluation)** (70 points total)\n",
    "    * **[10 points]** Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up\n",
    "any assertions.\n",
    "    * **[10 points]** Choose the method you will use for dividing your data into training and testing splits\n",
    "(i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "    * **[20 points]** Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "    * **[10 points]** Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "    * **[10 points]** Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "    * **[10 points]** Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "* **Deployment (5 points** total)\n",
    "    * **How useful is** your model for interested parties (i.e., the companies or organizations that might\n",
    "want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\n",
    "* **Exceptional Work (10** points total)\n",
    "    * **You have free** reign to provide additional analyses.\n",
    "    * **One idea:** grid search parameters in a parallelized fashion and visualize the performances across\n",
    "attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduciton\n",
    "Explanation of our data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation<a id=\"Data_Preperation\"></a>\n",
    "\n",
    "\n",
    "### [Pre Processing](#Pre_Processing)\n",
    "1. Set appropiate data types\n",
    "2. Removed columsn that may be problomatic\n",
    "    * Columns with same value in all rows\n",
    "    * Columsn with a unique value in all rows\n",
    "    * Empty Columns\n",
    "3. Remove columsn with low fill rate or high frequency of same value\n",
    "4. Missing values\n",
    "    * Update Census Continous values\n",
    "    * Update Census Categorical values\n",
    "    * Update remaining values\n",
    "5. Remove Duplicate Rows\n",
    "6. Data Quality Clean-up\n",
    "7. Create new features\n",
    "\n",
    "### [Feature Seleciton \\ Model Building](#Feature_Selection_Modeling_Building)\n",
    "1. Create One-Hot encoding on variables used for modeling\n",
    "2. Remove columsn with similar characterisitcs or a corrleation more than .95\n",
    "\n",
    "### [Data Definitions](#Data_Definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.19 s, sys: 395 ms, total: 5.59 s\n",
      "Wall time: 5.72 s\n",
      "CPU times: user 1.28 ms, sys: 181 µs, total: 1.46 ms\n",
      "Wall time: 1.4 ms\n"
     ]
    }
   ],
   "source": [
    "w\n",
    "\n",
    "%time malware = pd.read_csv(\"data/final.csv\")\n",
    "%time data_glossary = pd.read_csv(\"data/data_glossary.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_column_array(cols):\n",
    "    cols_booleans = [\n",
    "        \"IsBeta\",\n",
    "        \"IsSxsPassiveMode\",\n",
    "        \"HasTpm\",\n",
    "        \"IsProtected\",\n",
    "        \"AutoSampleOptIn\",\n",
    "        \"PuaMode\",\n",
    "        \"SMode\",\n",
    "        \"Firewall\",\n",
    "        \"UacLuaenable\",\n",
    "        \"Census_HasOpticalDiskDrive\",\n",
    "        \"Census_IsPortableOperatingSystem\",\n",
    "        \"Census_IsFlightingInternal\",\n",
    "        \"Census_IsFlightsDisabled\",\n",
    "        \"Census_ThresholdOptIn\",\n",
    "        \"Census_IsSecureBootEnabled\",\n",
    "        \"Census_IsWIMBootEnabled\",\n",
    "        \"Census_IsVirtualDevice\",\n",
    "        \"Census_IsTouchEnabled\",\n",
    "        \"Census_IsPenCapable\",\n",
    "        \"Census_IsAlwaysOnAlwaysConnectedCapable\",\n",
    "        \"Wdft_IsGamer\"\n",
    "    ]\n",
    "\n",
    "    cols_categorical = [\n",
    "        \"ProductName\",\n",
    "        \"EngineVersion\",\n",
    "        \"AppVersion\",\n",
    "        \"AvSigVersion_x_x\",\n",
    "        \"RtpStateBitfield\",\n",
    "        \"AVProductsInstalled\",\n",
    "        \"AVProductsEnabled\",\n",
    "        \"CountryIdentifier\",\n",
    "        \"OrganizationIdentifier\",\n",
    "        \"Platform\",\n",
    "        \"Processor\",\n",
    "        \"OsVer\",\n",
    "        \"OsBuild\",\n",
    "        \"OsSuite\",\n",
    "        \"OsPlatformSubRelease\",\n",
    "        \"SkuEdition\",\n",
    "        \"SmartScreen\",\n",
    "        \"Census_MDC2FormFactor\",\n",
    "        \"Census_DeviceFamily\",\n",
    "        \"Census_ProcessorManufacturerIdentifier\",\n",
    "        \"Census_ProcessorClass\",\n",
    "        \"Census_PrimaryDiskTypeName\",\n",
    "        \"Census_ChassisTypeName\",\n",
    "        \"Census_PowerPlatformRoleName\",\n",
    "        \"Census_InternalBatteryType\",\n",
    "        \"Census_OSArchitecture\",\n",
    "        \"Census_OSBranch\",\n",
    "        \"Census_OSBuildNumber\",\n",
    "        \"Census_OSEdition\",\n",
    "        \"Census_OSSkuName\",\n",
    "        \"Census_OSInstallTypeName\",\n",
    "        \"Census_OSInstallLanguageIdentifier\",\n",
    "        \"Census_OSUILocaleIdentifier\",\n",
    "        \"Census_OSWUAutoUpdateOptionsName\",\n",
    "        \"Census_GenuineStateName\",\n",
    "        \"Census_ActivationChannel\",\n",
    "        \"Census_FlightRing\",\n",
    "        \"Wdft_RegionIdentifier\"\n",
    "    ]\n",
    "\n",
    "    cols_categorical_large = [\n",
    "        \"AvSigVersion\",\n",
    "        \"DefaultBrowsersIdentifier\",\n",
    "        \"AVProductStatesIdentifier\",\n",
    "        \"CityIdentifier\",\n",
    "        \"GeoNameIdentifier\",\n",
    "        \"OsBuildLab\",\n",
    "        \"IeVerIdentifier\",\n",
    "        \"Census_OEMNameIdentifier\",\n",
    "        \"Census_OEMModelIdentifier\",\n",
    "        \"Census_ProcessorModelIdentifier\",\n",
    "        \"Census_OSVersion\",\n",
    "        \"Census_OSBuildRevision\",\n",
    "        \"Census_FirmwareManufacturerIdentifier\",\n",
    "        \"Census_FirmwareVersionIdentifier\",\n",
    "        \"LocaleEnglishNameIdentifier\"\n",
    "    ]\n",
    "\n",
    "    cols_numerical = [\n",
    "        \"Census_ProcessorCoreCount\",\n",
    "        \"Census_PrimaryDiskTotalCapacity\",\n",
    "        \"Census_SystemVolumeTotalCapacity\",\n",
    "        \"Census_TotalPhysicalRAM\",\n",
    "        \"Census_InternalPrimaryDiagonalDisplaySizeInInches\",\n",
    "        \"Census_InternalPrimaryDisplayResolutionHorizontal\",\n",
    "        \"Census_InternalPrimaryDisplayResolutionVertical\",\n",
    "        \"Census_InternalBatteryNumberOfCharges\"\n",
    "    ]\n",
    "    \n",
    "    # Update our column arrays\n",
    "    cols_categorical = [x for x in cols_categorical if x in cols]\n",
    "    cols_numerical = [x for x in cols_numerical if x in cols]\n",
    "    cols_booleans = [x for x in cols_booleans if x in cols]\n",
    "    cols_categorical_large = [x for x in cols_categorical_large if x in cols]\n",
    "    \n",
    "    return cols_categorical, cols_numerical, cols_booleans, cols_categorical_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing<a id=\"Pre_Processing\"></a>\n",
    "### 1. Mapping appropiate data types\n",
    "In order to understand the quality of the data, we did an extensive review of the data and determined which fields should be considered categorical, continous, or boolean.  We converted 23 id and category fields to object as well as removed outliers to make values boolean.  \n",
    "\n",
    "In order to help provide easier development, we created 4 array of column names: cols_booleans, cols_numerical, cols_categorical, and cols_categorical_large. The values with over 100 possible values were moved to its own bucket, cols_categorical_large, requiring special care to review in order to determine if we could cluster values together.\n",
    "\n",
    "The remaining code are steps we took to clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_booleans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['IsBeta',\n",
       " 'IsSxsPassiveMode',\n",
       " 'HasTpm',\n",
       " 'IsProtected',\n",
       " 'AutoSampleOptIn',\n",
       " 'PuaMode',\n",
       " 'SMode',\n",
       " 'Firewall',\n",
       " 'UacLuaenable',\n",
       " 'Census_HasOpticalDiskDrive',\n",
       " 'Census_IsPortableOperatingSystem',\n",
       " 'Census_IsFlightingInternal',\n",
       " 'Census_IsFlightsDisabled',\n",
       " 'Census_ThresholdOptIn',\n",
       " 'Census_IsSecureBootEnabled',\n",
       " 'Census_IsWIMBootEnabled',\n",
       " 'Census_IsVirtualDevice',\n",
       " 'Census_IsTouchEnabled',\n",
       " 'Census_IsPenCapable',\n",
       " 'Census_IsAlwaysOnAlwaysConnectedCapable',\n",
       " 'Wdft_IsGamer']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_numerical\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Census_ProcessorCoreCount',\n",
       " 'Census_PrimaryDiskTotalCapacity',\n",
       " 'Census_SystemVolumeTotalCapacity',\n",
       " 'Census_TotalPhysicalRAM',\n",
       " 'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n",
       " 'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
       " 'Census_InternalPrimaryDisplayResolutionVertical',\n",
       " 'Census_InternalBatteryNumberOfCharges']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_categorical\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ProductName',\n",
       " 'EngineVersion',\n",
       " 'AppVersion',\n",
       " 'RtpStateBitfield',\n",
       " 'AVProductsInstalled',\n",
       " 'AVProductsEnabled',\n",
       " 'CountryIdentifier',\n",
       " 'OrganizationIdentifier',\n",
       " 'Platform',\n",
       " 'Processor',\n",
       " 'OsVer',\n",
       " 'OsBuild',\n",
       " 'OsSuite',\n",
       " 'OsPlatformSubRelease',\n",
       " 'SkuEdition',\n",
       " 'SmartScreen',\n",
       " 'Census_MDC2FormFactor',\n",
       " 'Census_DeviceFamily',\n",
       " 'Census_ProcessorManufacturerIdentifier',\n",
       " 'Census_ProcessorClass',\n",
       " 'Census_PrimaryDiskTypeName',\n",
       " 'Census_ChassisTypeName',\n",
       " 'Census_PowerPlatformRoleName',\n",
       " 'Census_InternalBatteryType',\n",
       " 'Census_OSArchitecture',\n",
       " 'Census_OSBranch',\n",
       " 'Census_OSBuildNumber',\n",
       " 'Census_OSEdition',\n",
       " 'Census_OSSkuName',\n",
       " 'Census_OSInstallTypeName',\n",
       " 'Census_OSInstallLanguageIdentifier',\n",
       " 'Census_OSUILocaleIdentifier',\n",
       " 'Census_OSWUAutoUpdateOptionsName',\n",
       " 'Census_GenuineStateName',\n",
       " 'Census_ActivationChannel',\n",
       " 'Census_FlightRing',\n",
       " 'Wdft_RegionIdentifier']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_categorical_large\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AvSigVersion',\n",
       " 'DefaultBrowsersIdentifier',\n",
       " 'AVProductStatesIdentifier',\n",
       " 'CityIdentifier',\n",
       " 'GeoNameIdentifier',\n",
       " 'OsBuildLab',\n",
       " 'IeVerIdentifier',\n",
       " 'Census_OEMNameIdentifier',\n",
       " 'Census_OEMModelIdentifier',\n",
       " 'Census_ProcessorModelIdentifier',\n",
       " 'Census_OSVersion',\n",
       " 'Census_OSBuildRevision',\n",
       " 'Census_FirmwareManufacturerIdentifier',\n",
       " 'Census_FirmwareVersionIdentifier',\n",
       " 'LocaleEnglishNameIdentifier']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_categorical, cols_numerical, cols_booleans, cols_categorical_large = load_column_array(malware.columns)\n",
    "\n",
    "print( \"cols_booleans\" ) \n",
    "display( cols_booleans )\n",
    "print( \"cols_numerical\" )\n",
    "display( cols_numerical )\n",
    "print( \"cols_categorical\" )\n",
    "display( cols_categorical )\n",
    "print( \"cols_categorical_large\" )\n",
    "display( cols_categorical_large )\n",
    "\n",
    "#Convert features to right data type\n",
    "malware[cols_categorical] = malware[cols_categorical].astype(object)\n",
    "malware[cols_categorical_large] = malware[cols_categorical_large].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove problomatic columns\n",
    "* Columns with same value in all rows\n",
    "* Columsn with a unique value in all rows\n",
    "* Empty Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the following columns\n",
      " Index(['IsBeta', 'CountryIdentifier'], dtype='object')\n",
      "\n",
      "After: Removing columns with the same value in every row\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333411 entries, 0 to 333410\n",
      "Columns: 81 entries, MachineIdentifier to HasDetections\n",
      "dtypes: float64(19), int64(9), object(53)\n",
      "memory usage: 206.0+ MB\n",
      "\n",
      "Columns Deleted:  2\n",
      "Removing the following columns\n",
      " Index(['MachineIdentifier'], dtype='object')\n",
      "\n",
      "After: Removing columns with unique values in every row\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333411 entries, 0 to 333410\n",
      "Columns: 80 entries, ProductName to HasDetections\n",
      "dtypes: float64(19), int64(9), object(52)\n",
      "memory usage: 203.5+ MB\n",
      "\n",
      "Columns Deleted:  1\n",
      "Removing the following columns\n",
      " Index([], dtype='object')\n",
      "\n",
      "After: Removing columns with null / blank values in every row.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333411 entries, 0 to 333410\n",
      "Columns: 80 entries, ProductName to HasDetections\n",
      "dtypes: float64(19), int64(9), object(52)\n",
      "memory usage: 203.5+ MB\n",
      "\n",
      "Columns Deleted:  0\n"
     ]
    }
   ],
   "source": [
    "UniqueValueCounts = malware.nunique(dropna=False)\n",
    "SingleValueCols = UniqueValueCounts[UniqueValueCounts == 1].index\n",
    "malware = malware.drop(SingleValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print( \"Removing the following columns\\n\", SingleValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with the same value in every row\" )\n",
    "malware.info(verbose=False)\n",
    "print('\\r\\nColumns Deleted: ', len(SingleValueCols) )\n",
    "\n",
    "#Remove any fields that have unique values in every row\n",
    "malwareRecordCt = malware.shape[0]\n",
    "UniqueValueCounts = malware.apply(pd.Series.nunique)\n",
    "AllUniqueValueCols = UniqueValueCounts[UniqueValueCounts == malwareRecordCt].index\n",
    "malware = malware.drop(AllUniqueValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print( \"Removing the following columns\\n\", AllUniqueValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with unique values in every row\" )\n",
    "malware.info(verbose=False)\n",
    "print( \"\\r\\nColumns Deleted: \", len(AllUniqueValueCols) )\n",
    "\n",
    "#Remove any empty fields (null values in every row)\n",
    "malwareRecordCt = malware.shape[0]\n",
    "NullValueCounts = malware.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts == malwareRecordCt].index\n",
    "malware = malware.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print( \"Removing the following columns\\n\", NullValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with null / blank values in every row.\" )\n",
    "malware.info(verbose=False)\n",
    "print( \"\\r\\nColumns Deleted: \", len(NullValueCols) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Columns with low fill-rate or high frequency of same value\n",
    "\n",
    "* Remove columsn with 60% of values are NA or NULL\n",
    "* Remove columsn with 80% of high frequency of same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the following columns\n",
      " Index(['DefaultBrowsersIdentifier', 'PuaMode', 'Census_ProcessorClass',\n",
      "       'Census_InternalBatteryType', 'Census_IsFlightingInternal',\n",
      "       'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled'],\n",
      "      dtype='object')\n",
      "\r\n",
      "After: Removing columns with >= .6 % of missing values\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333411 entries, 0 to 333410\n",
      "Columns: 73 entries, ProductName to HasDetections\n",
      "dtypes: float64(16), int64(9), object(48)\n",
      "memory usage: 185.7+ MB\n",
      "\r\n",
      "Columns Deleted:  7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/GitHub/7331_Malware/ColumnArrays.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmalwareRecordCt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msameValueLimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalwareRecordCt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mMaxColumnFreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mHighFreqCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxColumnFreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMaxColumnFreq\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msameValueLimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmalware\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHighFreqCols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4875\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4876\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4877\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4878\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4879\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4931\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4932\u001b[0m                     result = lib.reduce(values, func, axis=axis, dummy=dummy,\n\u001b[0;32m-> 4933\u001b[0;31m                                         labels=labels)\n\u001b[0m\u001b[1;32m   4934\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4935\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/7331_Malware/ColumnArrays.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmalwareRecordCt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msameValueLimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalwareRecordCt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mMaxColumnFreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mHighFreqCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxColumnFreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMaxColumnFreq\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msameValueLimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmalware\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHighFreqCols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36mvalue_counts\u001b[0;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalue_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         result = value_counts(self, sort=sort, ascending=ascending,\n\u001b[0;32m--> 871\u001b[0;31m                               normalize=normalize, bins=bins, dropna=dropna)\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mvalue_counts\u001b[0;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_value_counts_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_value_counts_arraylike\u001b[0;34m(values, dropna)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# TODO: handle uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value_count_{dtype}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Eliminate columns with 80% of missing values\n",
    "malwareRecordCt = malware.shape[0]\n",
    "missingValueLimit = malwareRecordCt * .6\n",
    "NullValueCounts = malware.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts >= missingValueLimit].index\n",
    "malware = malware.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print( \"Removing the following columns\\n\", NullValueCols )\n",
    "print( \"\\r\\nAfter: Removing columns with >= .6 % of missing values\" )\n",
    "malware.info(verbose=False)\n",
    "print (\"\\r\\nColumns Deleted: \", len(NullValueCols) )\n",
    "\n",
    "#Eliminate values with 80% of the value is the same\n",
    "malwareRecordCt = malware.shape[0]\n",
    "sameValueLimit = malwareRecordCt * .8\n",
    "MaxColumnFreq = malware.apply(lambda x: x.value_counts().values[0])\n",
    "HighFreqCols = MaxColumnFreq[MaxColumnFreq >= sameValueLimit].index\n",
    "malware = malware.drop(HighFreqCols, axis=1)\n",
    "\n",
    "#Review dataset contents after high frequency delete\n",
    "print( \"Removing the following columns\\n\", HighFreqCols )\n",
    "print( \"\\r\\nAfter: Removing columns with >= .8 % of missing values\" )\n",
    "malware.info(verbose=False)\n",
    "print (\"\\r\\nColumns Deleted: \", len(HighFreqCols) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our column arrays\n",
    "cols_categorical = [x for x in cols_categorical if x in malware.columns]\n",
    "cols_numerical = [x for x in cols_numerical if x in malware.columns]\n",
    "cols_booleans = [x for x in cols_booleans if x in malware.columns]\n",
    "cols_categorical_large = [x for x in cols_categorical_large if x in malware.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Impute Missing Values\n",
    "#### Census Hardware Configurations\n",
    "The continuous values represent all of hardware configurations on a machine, for example memory and hard drive capacity.  In order to assign the right value, we will use the median value grouped by **Census_MDC2FormFactor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the median value for each field\n",
    "rowsBefore = malware[cols_numerical].isnull().T.any().T.sum()\n",
    "\n",
    "#Removing values less then 0\n",
    "malware[cols_numerical] = malware[cols_numerical].replace(-1, np.nan)\n",
    "\n",
    "malware[cols_numerical] = malware.groupby(\"Census_MDC2FormFactor\")[cols_numerical]\\\n",
    "    .transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "#Review dataset contents after Census_MDC2FormFactor Census hardware Imputation\n",
    "print(\"After: Updating Missing Continous Values\")   \n",
    "rowsAfter = malware[cols_numerical].isnull().T.any().T.sum()\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows Updated / Imputed: ', rowsUpdated )\n",
    "print('\\r\\nTotal Rows Missing Census Hardware by Census_MDC2FormFactor') \n",
    "malware['Census_MDC2FormFactor'][malware[cols_numerical].isnull().T.any().T].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Census Categorial Configurations\n",
    "Assign remaining missing values associated to Census details based on the mode of **Census_MDC2FormFactor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all census fields\n",
    "CensusFields = malware.filter(regex='Census').columns\n",
    "NullValueCounts = malware[CensusFields].isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts > 0 ].index\n",
    "\n",
    "rowsBefore = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "\n",
    "#Group by FormFactor and OSSkuName\n",
    "malware[NullValueCols] = malware.groupby([\"Census_MDC2FormFactor\"])[NullValueCols]\\\n",
    "    .transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "#Review dataset contents after Census Mode Imputations\n",
    "print(\"After: Updating Missing Continous Values\")   \n",
    "rowsAfter = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows Updated / Imputed: ', rowsUpdated )\n",
    "print('\\r\\nTotal Rows Missing for Census Fields: ',malware[CensusFields].isnull().T.any().T.sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remaining Features\n",
    "Set the remaining features as 0, to indicate not turned on (boolean) or feature available (categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NullValueCounts = malware.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts > 0 ].index\n",
    "\n",
    "rowsBefore = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "\n",
    "malware[NullValueCols] = malware[NullValueCols].fillna(0)\n",
    "\n",
    "#Review dataset contents after Census Mode Imputations\n",
    "print(\"After: Updating Missing Values\")   \n",
    "rowsAfter = malware[NullValueCols].isnull().T.any().T.sum()\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows Updated / Imputed: ', rowsUpdated )\n",
    "print('\\r\\nTotal Rows Missing values: ',malware.isnull().T.any().T.sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart Screen fill miising values and fix characters issue\n",
    "malware.SmartScreen.replace({\"off\":\"Off\",\"00000000\":\"ExistsNotSet\",\"&#x02;\" :\"ExistsNotSet\",\n",
    "                                 \"&#x01;\" :\"ExistsNotSet\",\"0\":\"ExistsNotSet\"},inplace=True)\n",
    "# currently renamed \"Census_PrimaryDiskTypeName\" unknown data into one category\n",
    "malware.Census_PrimaryDiskTypeName.replace({\"Unspecified\":\"Other\"},inplace=True)\n",
    "\n",
    "# currently renamed \"Census_ChassisTypeName\" unknown data into one category\n",
    "malware.Census_ChassisTypeName.replace({\"UNKNOWN\":\"Other\",\"Unknown\":\"Other\",\"0\" :\"Other\",\n",
    "                                \"30\" :\"Other\",\n",
    "                                \"35\" :\"Other\",\n",
    "                                \"112\" :\"Other\",\n",
    "                                \"76\" :\"Other\",\n",
    "                                \"39\" :\"Other\"},inplace=True)\n",
    "\n",
    "# currently renamed \"Census_PowerPlatformRoleName\" unknown data into one category\n",
    "\n",
    "malware.Census_PowerPlatformRoleName.fillna('Other', inplace=True)\n",
    "\n",
    "malware.Census_PowerPlatformRoleName.replace({\"UNKNOWN\":\"Other\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsBefore = len(malware)\n",
    "malware = malware.drop_duplicates()\n",
    "\n",
    "\n",
    "#Review dataset after deleting duplicates\n",
    "print(\"After: Deleting rows\")   \n",
    "rowsAfter = len(malware)\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print('Rows removed: ', rowsUpdated )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. New Features\n",
    "**AvSigVersion** is a version number with its format of *1.217.1014.0*.  To reduce our columnes for One-Hot Encoding, we reduced the version to just Major.Minor.  or *1.217*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvSigUniqueValueCounts = malware[\"AvSigVersion\"].nunique()\n",
    "\n",
    "AvSigVersion_split = malware[\"AvSigVersion\"].str.rsplit(pat=\".\",expand=True)\n",
    "malware[\"AvSigVersion_x_x\"] = AvSigVersion_split.loc[:,0]+\".\"+AvSigVersion_split.loc[:,1]\n",
    "\n",
    "AvSigXXUniqueValueCounts = malware[\"AvSigVersion_x_x\"].nunique()\n",
    "\n",
    "print( \"The unique values for AvSigVersion is \", AvSigUniqueValueCounts)\n",
    "print( \"After the transofmration of Major.Minor, the unique features reudced to \", AvSigXXUniqueValueCounts)\n",
    "\n",
    "# Add AvSigVersion_x_x to cols_categorical\n",
    "cols_categorical.append(\"AvSigVersion_x_x\")\n",
    "\n",
    "# Add to data glossary\n",
    "data_glossary = data_glossary.append(\n",
    "    pd.DataFrame(\n",
    "        data = [\"New Feature: Reduced version of Defender state information e.g. 1.217\"],\n",
    "        index = [\"AvSigVersion_x_x\"],\n",
    "        columns = [\"description\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our dataset as malware.clean.csv\n",
    "malware.to_csv(\"data/malware.clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection \\ Model Building<a id=\"Feature_Selection_Modeling_Building\"></a>\n",
    "Following functions are used during model building\n",
    "* Create Hot=One Encoding\n",
    "* Remove Highly Corrleated Features\n",
    "\n",
    "During our modeling, we will explore with scalling the data or PCA for feature reductions. **CHECK WITH DR. DREW WHY THIS IS REQUIRED IN DATA PREP????**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encodings(df, cols):\n",
    "    result = pd.DataFrame()\n",
    "    i = 0\n",
    "    for col in cols:\n",
    "        dummies = pd.get_dummies(df[col],prefix=col)\n",
    "        if( i == 0 ):\n",
    "            result = dummies.copy()\n",
    "        else:\n",
    "            result = pd.concat((result, dummies), axis=1)\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_features(df, verbose = False):\n",
    "#     # calculate the correlation matrix\n",
    "#     corr_matrix  = df.corr().abs()\n",
    "\n",
    "#     # Select upper triangle of correlation matrix\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "#     # Find index of feature columns with correlation greater than 0.95\n",
    "#     to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    \n",
    "#     #Get all of the correlation values > 95%\n",
    "#     x = np.where(upper > 0.95)\n",
    "\n",
    "#     #Display all field combinations with > 95% correlation\n",
    "#     cf = pd.DataFrame()\n",
    "#     cf['Field1'] = upper.columns[x[1]]\n",
    "#     cf['Field2'] = upper.index[x[0]]\n",
    "\n",
    "#     #Get the correlation values for every field combination. (There must be a more pythonic way to do this!)\n",
    "#     corr = [0] * len(cf)\n",
    "#     for i in range(0, len(cf)):\n",
    "#         corr[i] =  upper[cf['Field1'][i]][cf['Field2'][i]] \n",
    "\n",
    "#     cf['Correlation'] = corr\n",
    "\n",
    "#     if( verbose ):\n",
    "#         print('There are ', str(len(cf['Field1'])), ' field correlations > 95%.')\n",
    "#         display(cf)\n",
    "        \n",
    "#         print('Dropping the following ', str(len(to_drop)), ' highly correlated fields.')\n",
    "#         to_drop\n",
    "        \n",
    "#     #Check columns before drop \n",
    "#     if( verbose ):\n",
    "#         print('\\r\\n*********Before: Dropping Highly Correlated Fields*************************************')\n",
    "#         display(df.info(verbose=False))\n",
    "\n",
    "#     # Drop the highly correlated features from our training data \n",
    "#     df = df.drop(to_drop, axis=1)\n",
    "\n",
    "#     #Check columns after drop \n",
    "#     if( verbose ):\n",
    "#         print('\\r\\n*********After: Dropping Highly Correlated Fields**************************************')\n",
    "#         df.info(verbose=False)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of feature reductions on highly corrleated values\n",
    "1. Examine the columns\n",
    "    * We will create dummy variables for all columns that have less then 100 unique values, which are classified in the cols_categorical array.\n",
    "2. Remove highly correlated values\n",
    "\n",
    "During our modeling building we will include the larger categorical variables as needed in order to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# malware = pd.read_csv(\"data/malware.clean.csv\")\n",
    "# malware[cols_categorical] = malware[cols_categorical].astype(object)\n",
    "# malware[cols_categorical_large] = malware[cols_categorical_large].astype(object)\n",
    "\n",
    "# oneHotUniqueValueCounts = malware[cols_categorical].apply(lambda x: x.nunique())\n",
    "# print( \"Cateogiries that have less then 100 unique values\" )\n",
    "# display(oneHotUniqueValueCounts)\n",
    "\n",
    "# oneHotUniqueValueCounts = malware[cols_categorical_large].apply(lambda x: x.nunique())\n",
    "# print( \"Cateogiries that have more then 50 unique values\" )\n",
    "# display(oneHotUniqueValueCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.concat(\n",
    "    (    \n",
    "        malware[cols_booleans],\n",
    "        malware[cols_numerical],\n",
    "        get_one_hot_encodings(malware, cols_categorical)\n",
    "    ), axis = 1)\n",
    "\n",
    "print(\"Total Features after One-Hot Encoding: \", model_data.shape )\n",
    "print(\"Reduce Features based on highly correlated values > .95\")\n",
    "model_data = reduce_features(model_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our dataset as malware.clean.dummy.csv\n",
    "model_data.to_csv(\"data/malware.clean.dummy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definitions<a id=\"Data_Definitions\"></a>\n",
    "\n",
    "The following values are the remaining fields we will use after removing columns we found problamatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 120)\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "# Only display fields remaining in malware\n",
    "display( data_glossary.loc[malware.columns,:] )\n",
    "\n",
    "pd.reset_option('max_colwidth')\n",
    "pd.reset_option('max_row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation <a id=\"Modeling_and_Evaluation\"></a>\n",
    "1. [Evaluation Metrics](#Evaluation_Metrics)\n",
    "2. [Training Methods](#Training_Methods)\n",
    "3. [Classification Models](#Classification_Models)\n",
    "    * [Support Vector Machine](#Support_Vector_Machine)\n",
    "    * [Random Forest](#Random_Forest)\n",
    "    * [K-Nearest Neighbors](#K_Nearest_Neighbors)\n",
    "4. Model Compersion\n",
    "5. Model Advantages\n",
    "6. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Different Training Methods (K-Fold, Stratified K-Fold, ShuffleSplit, and Stratified Shuffle Split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below study is analysis of different splits of different training method. For visualization of different training methods, the data set is sorted by \"HasDetections\" as class to differentiate different method. Based on this study, the best training method is ShuffleSplit. Using Shuffle Split, we can see that the training model is randomized to make up 80-20 training-test split. Each cross validation, different data set (training/test) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Later\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "GB = [1, 2, 3, 4]\n",
    "loo = LeaveOneOut()\n",
    "for train, test in loo.split(GB):\n",
    "    print(\"%s %s\" % (train, test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our dataset as malware.clean.csv\n",
    "# malware.to_csv(\"data/malware.clean.csv\", index=False)\n",
    "malware = pd.read_csv(\"data/malware.clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encodings(df, cols):\n",
    "    result = pd.DataFrame()\n",
    "    i = 0\n",
    "    for col in cols:\n",
    "        dummies = pd.get_dummies(df[col],prefix=col)\n",
    "        if( i == 0 ):\n",
    "            result = dummies.copy()\n",
    "        else:\n",
    "            result = pd.concat((result, dummies), axis=1)\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_features(df, verbose = False):\n",
    "    # calculate the correlation matrix\n",
    "    corr_matrix  = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    \n",
    "    #Get all of the correlation values > 95%\n",
    "    x = np.where(upper > 0.95)\n",
    "\n",
    "    #Display all field combinations with > 95% correlation\n",
    "    cf = pd.DataFrame()\n",
    "    cf['Field1'] = upper.columns[x[1]]\n",
    "    cf['Field2'] = upper.index[x[0]]\n",
    "\n",
    "    #Get the correlation values for every field combination. (There must be a more pythonic way to do this!)\n",
    "    corr = [0] * len(cf)\n",
    "    for i in range(0, len(cf)):\n",
    "        corr[i] =  upper[cf['Field1'][i]][cf['Field2'][i]] \n",
    "\n",
    "    cf['Correlation'] = corr\n",
    "\n",
    "    if( verbose ):\n",
    "        print('There are ', str(len(cf['Field1'])), ' field correlations > 95%.')\n",
    "        display(cf)\n",
    "        \n",
    "        print('Dropping the following ', str(len(to_drop)), ' highly correlated fields.')\n",
    "        to_drop\n",
    "        \n",
    "    #Check columns before drop \n",
    "    if( verbose ):\n",
    "        print('\\r\\n*********Before: Dropping Highly Correlated Fields*************************************')\n",
    "        display(df.info(verbose=False))\n",
    "\n",
    "    # Drop the highly correlated features from our training data \n",
    "    df = df.drop(to_drop, axis=1)\n",
    "\n",
    "    #Check columns after drop \n",
    "    if( verbose ):\n",
    "        print('\\r\\n*********After: Dropping Highly Correlated Fields**************************************')\n",
    "        df.info(verbose=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['ProductName' 'RtpStateBitfield' 'AVProductsEnabled' 'CountryIdentifier'\\n 'Platform' 'Processor' 'OsVer' 'Census_DeviceFamily'\\n 'Census_ProcessorClass' 'Census_InternalBatteryType'\\n 'Census_OSArchitecture' 'Census_GenuineStateName' 'Census_FlightRing'\\n 'Wdft_RegionIdentifier'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/GitHub/7331_Malware/ColumnArrays.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmalware\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/malware.clean.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical_large\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical_large\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moneHotUniqueValueCounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalware\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_categorical\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['ProductName' 'RtpStateBitfield' 'AVProductsEnabled' 'CountryIdentifier'\\n 'Platform' 'Processor' 'OsVer' 'Census_DeviceFamily'\\n 'Census_ProcessorClass' 'Census_InternalBatteryType'\\n 'Census_OSArchitecture' 'Census_GenuineStateName' 'Census_FlightRing'\\n 'Wdft_RegionIdentifier'] not in index\""
     ]
    }
   ],
   "source": [
    "malware = pd.read_csv(\"data/malware.clean.csv\")\n",
    "malware[cols_categorical] = malware[cols_categorical].astype(object)\n",
    "malware[cols_categorical_large] = malware[cols_categorical_large].astype(object)\n",
    "\n",
    "oneHotUniqueValueCounts = malware[cols_categorical].apply(lambda x: x.nunique())\n",
    "print( \"Cateogiries that have less then 100 unique values\" )\n",
    "display(oneHotUniqueValueCounts)\n",
    "\n",
    "oneHotUniqueValueCounts = malware[cols_categorical_large].apply(lambda x: x.nunique())\n",
    "print( \"Cateogiries that have more then 50 unique values\" )\n",
    "display(oneHotUniqueValueCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is GB's Attempt at RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean absolute error (MAE) to score the regression models created \n",
    "#(the scale of MAE is identical to the response variable)\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer\n",
    "               } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 120)\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "pd.reset_option('max_colwidth')\n",
    "pd.reset_option('max_row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1.1.15200.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/GitHub/7331_Malware/ColumnArrays.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    647\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    648\u001b[0m                         \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '1.1.15200.1'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "y = malware[\"HasDetections\"].values\n",
    "malware_ND = malware\n",
    "del malware_ND['HasDetections']\n",
    "X = malware_ND.values\n",
    "\n",
    "scl = StandardScaler()\n",
    "X = scl.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1.1.15200.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/GitHub/7331_Malware/ColumnArrays.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# NOTE: you can parallelize this using the cross_val_predict method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0myhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \"\"\"\n\u001b[1;32m    890\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '1.1.15200.1'"
     ]
    }
   ],
   "source": [
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "for train, test in cv.split(X,y):\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('KNN accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/GitHub/7331_Malware/ColumnArrays.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #Divide data into test and training splits\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "#  = ShuffleSplit(n_splits=5, test_size=0.20, random_state=0)\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "LeaveOneOut()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "linreg = RandomForestRegressor()\n",
    "parameters = { 'min_samples_split':[2,3,4,5]\n",
    "              ,'n_estimators' : [500]\n",
    "              ,'min_samples_leaf': [10, 25, 50]\n",
    "              ,'criterion': ['mae']\n",
    "              ,'n_jobs':[8] \n",
    "              ,'random_state': [0]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , n_jobs=8 \n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "# #Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA / Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',PCA(n_components=100, svd_solver='randomized')),\n",
    "     ('CLF',RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)\n",
    "plot_class_acc(y,yhat,title=\"Random Forest + PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "plot_class_acc(y,yhat,title=\"Random Forest, Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets get access to the different properties of our RF\n",
    "\n",
    "print (clf)\n",
    "\n",
    "plt.barh(range(len(clf.feature_importances_)), clf.feature_importances_)\n",
    "plt.show()\n",
    "\n",
    "print ('Generalization score estimate from training data', clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Ensemble Comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_estimators = 50\n",
    "# lets train some trees\n",
    "clf_array = [\n",
    "    ('Stump',              DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)),\n",
    "    ('Tree',               DecisionTreeClassifier()),\n",
    "    ('Random Trees',       RandomForestClassifier(max_depth=50, n_estimators=num_estimators)),\n",
    "    ('Extra Random Trees', ExtraTreesClassifier(n_estimators=num_estimators,min_samples_split=2)),\n",
    "    ('Boosted Tree',       GradientBoostingClassifier(n_estimators=num_estimators)), #takes a long time\n",
    "    ]\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X,y)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# setup pipeline to take PCA, then fit a different classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',PCA(n_components=100,svd_solver='randomized')),\n",
    "     ('CLF',GaussianNB())]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)\n",
    "plot_class_acc(y,yhat,title=\"Naive Bayes + PCA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
