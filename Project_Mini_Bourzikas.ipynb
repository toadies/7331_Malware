{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 7331 Data Mining\n",
    "### Logistic Regression and SVM\n",
    "### Mini Lab\n",
    "* Tahir Ahmad<br>\n",
    "* Christopher Ballenger<br>\n",
    "* Grant Bourzikas<br>\n",
    "* Vitaly Briker<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "%run -i ColumnArrays-Grant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.03 s, sys: 271 ms, total: 4.3 s\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%time final = pd.read_csv(\"data/clean.final.csv\")\n",
    "final[cols_categorical] = final[cols_categorical].astype(object)\n",
    "final[cols_categorical_large] = final[cols_categorical_large].astype(object)\n",
    "final[cols_booleans] = final[cols_booleans].astype(int)\n",
    "final[cols_numerical] = final[cols_numerical].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables\n",
    "Created Dummy Variables for all the cols_categorical variables\n",
    "How to use it.\n",
    "\n",
    "Merge any Response with a variable\n",
    "```python\n",
    "df = pd.concat((Response,EngineVersion, Census_MDC2FormFactor),axis=1)\n",
    "```\n",
    "**Do not sort result or reset index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Response = final[[\"HasDetections\", \"MachineIdentifier\"]]\n",
    "EngineVersion = pd.get_dummies(final[\"EngineVersion\"],prefix=\"EngineVersion\")\n",
    "RtpStateBitfield = pd.get_dummies(final[\"RtpStateBitfield\"],prefix=\"RtpStateBitfield\")\n",
    "AVProductsInstalled = pd.get_dummies(final[\"AVProductsInstalled\"],prefix=\"AVProductsInstalled\")\n",
    "AVProductsEnabled = pd.get_dummies(final[\"AVProductsEnabled\"],prefix=\"AVProductsEnabled\")\n",
    "OrganizationIdentifier = pd.get_dummies(final[\"OrganizationIdentifier\"],prefix=\"OrganizationIdentifier\")\n",
    "Platform = pd.get_dummies(final[\"Platform\"],prefix=\"Platform\")\n",
    "Processor = pd.get_dummies(final[\"Processor\"],prefix=\"Processor\")\n",
    "OsVer = pd.get_dummies(final[\"OsVer\"],prefix=\"OsVer\")\n",
    "OsBuild = pd.get_dummies(final[\"OsBuild\"],prefix=\"OsBuild\")\n",
    "OsSuite = pd.get_dummies(final[\"OsSuite\"],prefix=\"OsSuite\")\n",
    "OsPlatformSubRelease = pd.get_dummies(final[\"OsPlatformSubRelease\"],prefix=\"OsPlatformSubRelease\")\n",
    "SkuEdition = pd.get_dummies(final[\"SkuEdition\"],prefix=\"SkuEdition\")\n",
    "SmartScreen = pd.get_dummies(final[\"SmartScreen\"],prefix=\"SmartScreen\")\n",
    "Census_MDC2FormFactor = pd.get_dummies(final[\"Census_MDC2FormFactor\"],prefix=\"Census_MDC2FormFactor\")\n",
    "Census_ProcessorManufacturerIdentifier = pd.get_dummies(final[\"Census_ProcessorManufacturerIdentifier\"],prefix=\"Census_ProcessorManufacturerIdentifier\")\n",
    "Census_PrimaryDiskTypeName = pd.get_dummies(final[\"Census_PrimaryDiskTypeName\"],prefix=\"Census_PrimaryDiskTypeName\")\n",
    "Census_ChassisTypeName = pd.get_dummies(final[\"Census_ChassisTypeName\"],prefix=\"Census_ChassisTypeName\")\n",
    "Census_PowerPlatformRoleName = pd.get_dummies(final[\"Census_PowerPlatformRoleName\"],prefix=\"Census_PowerPlatformRoleName\")\n",
    "Census_OSArchitecture = pd.get_dummies(final[\"Census_OSArchitecture\"],prefix=\"Census_OSArchitecture\")\n",
    "Census_OSBranch = pd.get_dummies(final[\"Census_OSBranch\"],prefix=\"Census_OSBranch\")\n",
    "Census_OSBuildNumber = pd.get_dummies(final[\"Census_OSBuildNumber\"],prefix=\"Census_OSBuildNumber\")\n",
    "Census_OSEdition = pd.get_dummies(final[\"Census_OSEdition\"],prefix=\"Census_OSEdition\")\n",
    "Census_OSSkuName = pd.get_dummies(final[\"Census_OSSkuName\"],prefix=\"Census_OSSkuName\")\n",
    "Census_OSInstallTypeName = pd.get_dummies(final[\"Census_OSInstallTypeName\"],prefix=\"Census_OSInstallTypeName\")\n",
    "Census_OSInstallLanguageIdentifier = pd.get_dummies(final[\"Census_OSInstallLanguageIdentifier\"],prefix=\"Census_OSInstallLanguageIdentifier\")\n",
    "Census_OSUILocaleIdentifier = pd.get_dummies(final[\"Census_OSUILocaleIdentifier\"],prefix=\"Census_OSUILocaleIdentifier\")\n",
    "Census_OSWUAutoUpdateOptionsName = pd.get_dummies(final[\"Census_OSWUAutoUpdateOptionsName\"],prefix=\"Census_OSWUAutoUpdateOptionsName\")\n",
    "Census_GenuineStateName = pd.get_dummies(final[\"Census_GenuineStateName\"],prefix=\"Census_GenuineStateName\")\n",
    "Census_ActivationChannel = pd.get_dummies(final[\"Census_ActivationChannel\"],prefix=\"Census_ActivationChannel\")\n",
    "Census_FlightRing = pd.get_dummies(final[\"Census_FlightRing\"],prefix=\"Census_FlightRing\")\n",
    "Wdft_RegionIdentifier = pd.get_dummies(final[\"Wdft_RegionIdentifier\"],prefix=\"Wdft_RegionIdentifier\")\n",
    "\n",
    "# # Include Large Cat\n",
    "# AppVersion = pd.get_dummies(final[\"AppVersion\"],prefix=\"AppVersion\")\n",
    "# AvSigVersion = pd.get_dummies(final[\"AvSigVersion\"],prefix=\"AvSigVersion\")\n",
    "# AVProductStatesIdentifier = pd.get_dummies(final[\"AVProductStatesIdentifier\"],prefix=\"AVProductStatesIdentifier\")\n",
    "# CityIdentifier = pd.get_dummies(final[\"CityIdentifier\"],prefix=\"CityIdentifier\")\n",
    "# GeoNameIdentifier = pd.get_dummies(final[\"GeoNameIdentifier\"],prefix=\"GeoNameIdentifier\")\n",
    "# OsBuildLab = pd.get_dummies(final[\"OsBuildLab\"],prefix=\"OsBuildLab\")\n",
    "# IeVerIdentifier = pd.get_dummies(final[\"IeVerIdentifier\"],prefix=\"IeVerIdentifier\")\n",
    "# Census_OEMNameIdentifier = pd.get_dummies(final[\"Census_OEMNameIdentifier\"],prefix=\"Census_OEMNameIdentifier\")\n",
    "# Census_OEMModelIdentifier = pd.get_dummies(final[\"Census_OEMModelIdentifier\"],prefix=\"Census_OEMModelIdentifier\")\n",
    "# Census_ProcessorModelIdentifier = pd.get_dummies(final[\"Census_ProcessorModelIdentifier\"],prefix=\"Census_ProcessorModelIdentifier\")\n",
    "# Census_OSVersion = pd.get_dummies(final[\"Census_OSVersion\"],prefix=\"Census_OSVersion\")\n",
    "# Census_OSBuildRevision = pd.get_dummies(final[\"Census_OSBuildRevision\"],prefix=\"Census_OSBuildRevision\")\n",
    "# Census_FirmwareManufacturerIdentifier = pd.get_dummies(final[\"Census_FirmwareManufacturerIdentifier\"],prefix=\"Census_FirmwareManufacturerIdentifier\")\n",
    "# Census_FirmwareVersionIdentifier = pd.get_dummies(final[\"Census_FirmwareVersionIdentifier\"],prefix=\"Census_FirmwareVersionIdentifier\")\n",
    "# display(Response.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.concat((Response,EngineVersion, Census_MDC2FormFactor),axis=1)\n",
    "# df = pd.concat((Response,EngineVersion, Census_MDC2FormFactor, Census_FirmwareVersionIdentifier, Census_FirmwareManufacturerIdentifier),axis=1)\n",
    "\n",
    "# Validate a couple\n",
    "# display(final.loc[final.MachineIdentifier.isin([\n",
    "#     \"000046e59c37136173428e560acbe3a2\",\n",
    "#     \"0000d738ef47f088c53fef6013268ac1\",\n",
    "#     \"ffff192b25a721196450283732616ef8\",\n",
    "#     \"ffff4e2671cb933424fdf6f6b237cce3\"]),\n",
    "#     [\"MachineIdentifier\",\"HasDetections\",\"EngineVersion\",\"Census_MDC2FormFactor\", \"Census_FirmwareVersionIdentifier\"]])\n",
    "\n",
    "# display(df.loc[final.MachineIdentifier.isin([\n",
    "#     \"000046e59c37136173428e560acbe3a2\",\n",
    "#     \"0000d738ef47f088c53fef6013268ac1\",\n",
    "#     \"ffff192b25a721196450283732616ef8\",\n",
    "#     \"ffff4e2671cb933424fdf6f6b237cce3\"]),:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=1, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "y = Response['HasDetections'].values # get the labels we want\n",
    "X = pd.concat((\n",
    "        final[cols_booleans],\n",
    "        EngineVersion,\n",
    "        RtpStateBitfield,\n",
    "        AVProductsInstalled,\n",
    "        AVProductsEnabled,\n",
    "        OrganizationIdentifier,\n",
    "        Platform,\n",
    "        Processor,\n",
    "        OsVer,\n",
    "        OsBuild,\n",
    "        OsSuite,\n",
    "        OsPlatformSubRelease,\n",
    "        SkuEdition,\n",
    "        SmartScreen,\n",
    "        Census_MDC2FormFactor,\n",
    "        Census_ProcessorManufacturerIdentifier,\n",
    "        Census_PrimaryDiskTypeName,\n",
    "        Census_ChassisTypeName,\n",
    "        Census_PowerPlatformRoleName,\n",
    "        Census_OSArchitecture,\n",
    "        Census_OSBranch,\n",
    "        Census_OSBuildNumber,\n",
    "        Census_OSEdition,\n",
    "        Census_OSSkuName,\n",
    "        Census_OSInstallTypeName,\n",
    "        Census_OSInstallLanguageIdentifier,\n",
    "        Census_OSUILocaleIdentifier,\n",
    "        Census_OSWUAutoUpdateOptionsName,\n",
    "        Census_GenuineStateName,\n",
    "        Census_ActivationChannel,\n",
    "        Census_FlightRing,\n",
    "        Wdft_RegionIdentifier\n",
    "#         AppVersion,\n",
    "#         AvSigVersion,\n",
    "#         AVProductStatesIdentifier,\n",
    "#         CityIdentifier,\n",
    "#         GeoNameIdentifier,\n",
    "#         OsBuildLab,\n",
    "#         IeVerIdentifier,\n",
    "#         Census_OEMNameIdentifier,\n",
    "#         Census_OEMModelIdentifier,\n",
    "#         Census_ProcessorModelIdentifier,\n",
    "#         Census_OSVersion,\n",
    "#         Census_OSBuildRevision,\n",
    "#         Census_FirmwareManufacturerIdentifier,\n",
    "#         Census_FirmwareVersionIdentifier\n",
    "    ),axis=1).values\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 1\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbourzik/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = final.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,final.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=final.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
